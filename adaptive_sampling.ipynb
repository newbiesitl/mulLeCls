{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from data set\n",
    "from utils.data_utils import jigsaw_toxix_ds_get_df\n",
    "import numpy as np\n",
    "from config import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = jigsaw_toxix_ds_get_df()\n",
    "comments = df[\"comment_text\"].tolist()\n",
    "comments = [x.replace('\\n', ' ') for x in comments]\n",
    "\n",
    "classes = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "dump_text = '\\n'.join(comments)\n",
    "total_classes = 6\n",
    "class_matrix = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].astype('int')\n",
    "label_matrix = class_matrix.values\n",
    "h_dim = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 6.66666667,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  3.33333333]), array([ 1. ,  1.1,  1.2,  1.3,  1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2. ]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained w2v...done.\n"
     ]
    }
   ],
   "source": [
    "# load word2vec model\n",
    "import os \n",
    "from config import model_folder\n",
    "from model_factory.embeddings.w2v import w2v_load_from_keyedvectors, build_embedding_layer\n",
    "w2v_name = 'google_keyed_vector_format.bin'\n",
    "model_path = os.path.join(model_folder, w2v_name)\n",
    "print('loading pretrained w2v', end='...')    \n",
    "w2v_model = w2v_load_from_keyedvectors(model_path)\n",
    "vocab = w2v_model.vocab\n",
    "print('done.')\n",
    "word_2_idx = dict(zip(vocab.keys(), range(len(vocab))))\n",
    "max_sent_length = 80\n",
    "trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing data...done.\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "import nltk\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def tokenizer(text, word2idx, max_len=80, total=None):\n",
    "    if total is not None:\n",
    "        text = text[:total]\n",
    "    for sentence_idx in range(len(text)):\n",
    "        sentence = text[sentence_idx]\n",
    "        text[sentence_idx] = nltk.word_tokenize(sentence)\n",
    " \n",
    "    def _sent_to_idx(s, w2i):\n",
    "        for word_idx in range(len(s)):\n",
    "            word = s[word_idx]\n",
    "            idx = w2i.get(word, w2i['null'])\n",
    "            s[word_idx] = idx\n",
    "        return s\n",
    "        \n",
    "    for sentence_idx in range(len(text)):\n",
    "        sentence = text[sentence_idx]\n",
    "        sequence = _sent_to_idx(sentence, word2idx)\n",
    "        text[sentence_idx] = sequence\n",
    "    \n",
    "    text = pad_sequences(text, maxlen=max_len, value=word2idx['null'])   \n",
    "    return text\n",
    "\n",
    "num_samples = 5000\n",
    "print('tokenizing data', end='...')\n",
    "tokenized_sequence = tokenizer(comments, word_2_idx, max_sent_length, total=num_samples)\n",
    "if num_samples is not None:\n",
    "    label_matrix = class_matrix.values[:num_samples]\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   Created by Yubo Zhou on 28/03/19\n",
    "'''\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy as np\n",
    "\n",
    "class SeqCLS(object):\n",
    "    def __init__(self):\n",
    "        self.m = None\n",
    "        self.model_t = None\n",
    "        self.num_classes = 0\n",
    "\n",
    "    def configure(self, input_dim, seq_len, output_dim, h_dim, dropout=0.5,\n",
    "                  loss=binary_crossentropy, pretrained_embedding=None,\n",
    "                  verbose=0,\n",
    "                  ):\n",
    "        self.num_classes = output_dim\n",
    "        # with tf.device('/cpu:0'):\n",
    "        if True:\n",
    "            m = keras.models.Sequential()\n",
    "            if pretrained_embedding is None:\n",
    "                lstm_layer = keras.layers.LSTM(\n",
    "                    input_shape=(seq_len, input_dim),\n",
    "                    return_sequences=False,\n",
    "                    units=h_dim,\n",
    "                    dropout=dropout, recurrent_dropout=dropout,\n",
    "                )\n",
    "                m.add(lstm_layer)\n",
    "                if verbose:\n",
    "                    m.summary()\n",
    "            else:\n",
    "                m.add(pretrained_embedding)\n",
    "                m.add(\n",
    "                    keras.layers.LSTM(\n",
    "                        return_sequences=False,\n",
    "                        units=h_dim,\n",
    "                        dropout=dropout, recurrent_dropout=dropout,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        dense_h = keras.layers.Dense(\n",
    "            units=h_dim,\n",
    "            activation='selu',\n",
    "        )\n",
    "        m.add(dense_h); m.add(keras.layers.AlphaDropout(0.5))\n",
    "        m.add(\n",
    "            keras.layers.Dense(\n",
    "                units=self.num_classes,\n",
    "                activation='sigmoid',\n",
    "\n",
    "            )\n",
    "        )\n",
    "        m.compile(loss=loss, optimizer='adam')\n",
    "        self.m = m\n",
    "        tensors = K.function([self.m.layers[0].input, K.learning_phase()],\n",
    "                                          [self.m.layers[-1].output])\n",
    "        self.model_t = tensors\n",
    "        if verbose:\n",
    "            self.m.summary()\n",
    "\n",
    "    def fit(self, X, Y, epochs=50, batch_size=32, validation_split=.0, shuffle=True, verbose=2):\n",
    "        self.m.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=validation_split,\n",
    "                   shuffle=shuffle, verbose=verbose)\n",
    "\n",
    "        tensors = K.function([self.m.layers[0].input, K.learning_phase()],\n",
    "                                          [self.m.layers[-1].output])\n",
    "        self.model_t = tensors\n",
    "\n",
    "\n",
    "    def predict_with_uncertainty(self, X, sim=1):\n",
    "        result = self.sample_output(X, n_iter=sim)\n",
    "        prediction = self.m.predict(X)\n",
    "        result_cpy = np.swapaxes(result, 0, 1)\n",
    "        result_cpy = np.swapaxes(result_cpy, 1, 2)\n",
    "        certainties = np.zeros((result_cpy.shape[0], result_cpy.shape[1]))\n",
    "        for data_idx in range(result_cpy.shape[0]):\n",
    "            for topic_idx in range(result_cpy[data_idx].shape[0]):\n",
    "                samples = result_cpy[data_idx][topic_idx]\n",
    "                ret = np.histogram(samples, normed=True)\n",
    "                bins, bin_edges = ret\n",
    "                norm_bins = bins/np.sum(bins)\n",
    "                \n",
    "                certainty_score = 0\n",
    "                for idx in range(len(norm_bins)):\n",
    "                    if bin_edges[idx] < prediction[data_idx][topic_idx] <= bin_edges[idx+1]:\n",
    "                        certainty_score = norm_bins[idx]\n",
    "                certainties[data_idx][topic_idx] = certainty_score\n",
    "                print(prediction[data_idx][topic_idx], bin_edges, certainty_score)\n",
    "                break\n",
    "            break\n",
    "        return prediction, uncertainties\n",
    "\n",
    "    def sample_output(self, X, n_iter=1):\n",
    "        result = np.zeros((n_iter,) + (X.shape[0], self.num_classes))\n",
    "        for i in range(n_iter):\n",
    "            result[i, :, :] = self.model_t((X, 1))[0]\n",
    "        return result\n",
    "\n",
    "    def summary(self):\n",
    "        self.m.summary()\n",
    "        \n",
    "def eval_model(m, test_X, test_Y, sim=10, threshold = 0.5):\n",
    "    pred_Y, uncertainty_Y = m.predict_with_uncertainty(test_X, sim=sim)\n",
    "    topic_Y = test_Y.T\n",
    "    pred_topic_Y = pred_Y.T\n",
    "    # evaluate performance\n",
    "    \n",
    "    print(threshold)\n",
    "    print(','.join(['sample size','precision','recall', 'prior']))\n",
    "    # print(pred_topic_Y[0].tolist())\n",
    "    for topic_idx in range(len(pred_topic_Y)):\n",
    "        true_topic = topic_Y[topic_idx]\n",
    "        pred_topic = pred_topic_Y[topic_idx]\n",
    "        pred_topic[np.where(pred_topic >= threshold)] = 1\n",
    "        pred_topic[np.where(pred_topic < threshold)] = 0\n",
    "        print('%d, %.2f, %.2f, %.2f' \n",
    "              % (sum(true_topic), \n",
    "                 precision_score(true_topic, pred_topic, average='binary'), \n",
    "                 recall_score(true_topic, pred_topic), \n",
    "                 sum(true_topic)/len(true_topic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize embedding layer...done.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 80, 300)           900000000 \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "alpha_dropout_1 (AlphaDropou (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1806      \n",
      "=================================================================\n",
      "Total params: 900,813,306\n",
      "Trainable params: 813,306\n",
      "Non-trainable params: 900,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# bayes network\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def get_new_model(w2v_model):\n",
    "    print('initialize embedding layer', end='...')\n",
    "    embedding_layer = build_embedding_layer(w2v_model, word_2_idx, \n",
    "                                            len(vocab), max_sent_length, trainable)\n",
    "    print('done.')\n",
    "    m = SeqCLS()\n",
    "    m.configure(None, \n",
    "                None, \n",
    "                total_classes, 300, \n",
    "                pretrained_embedding=embedding_layer,\n",
    "                verbose=1,\n",
    "                )\n",
    "    return m\n",
    "\n",
    "\n",
    "            \n",
    "model_copy = get_new_model(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0221306 [ 0.08812507  0.09960809  0.1110911   0.12257411  0.13405712  0.14554014\n",
      "  0.15702315  0.16850616  0.17998918  0.19147219  0.2029552 ]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'uncertainty_score' referenced before assignment",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a88153ce864a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mtotal_Y\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mselected_Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mbatch_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c68905ed09aa>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(m, test_X, test_Y, sim, threshold)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mpred_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muncertainty_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_with_uncertainty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mtopic_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mpred_topic_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c68905ed09aa>\u001b[0m in \u001b[0;36mpredict_with_uncertainty\u001b[0;34m(self, X, sim)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbin_edges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mbin_edges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                         \u001b[0muncertainty_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_bins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0muncertainties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muncertainty_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muncertainties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'uncertainty_score' referenced before assignment"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# importance sampling\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from sklearn.metrics.classification import precision_score, recall_score\n",
    "\n",
    "batch_size = 5000\n",
    "batch_idx = 0\n",
    "train_test_split = 0.9\n",
    "divider = int(len(tokenized_sequence) * train_test_split)\n",
    "train_X, train_Y = tokenized_sequence[:divider], label_matrix[:divider]\n",
    "test_X, test_Y = tokenized_sequence[divider:], label_matrix[divider:]\n",
    "total_data = train_X.shape[0]\n",
    "sim = 10\n",
    "total_X = []\n",
    "total_Y = []\n",
    "\n",
    "\n",
    "print(batch_size)\n",
    "target_topic_idx = 0\n",
    "\n",
    "while True:\n",
    "    l, r = batch_idx*batch_size, min((batch_idx+1)*batch_size, total_data-1)\n",
    "    if l > r:\n",
    "        batch_idx = 0\n",
    "        break\n",
    "    this_batch_indices = np.array(range(l, r))\n",
    "    selected_batch = train_X[this_batch_indices]\n",
    "#     reset_weights()\n",
    "    if len(total_X) > 0:        \n",
    "        model_copy.fit(\n",
    "            np.array(total_X), \n",
    "            np.array(total_Y), \n",
    "            epochs=20, batch_size=100, verbose=0)\n",
    "    pred_Y, certainty_Y = model_copy.predict_with_uncertainty(selected_batch, sim=sim)\n",
    "    \n",
    "\n",
    "    certainty_Y_cpy = np.swapaxes(certainty_Y, 0, -1)\n",
    "    \n",
    "    for topic_index in range(certainty_Y_cpy.shape[0]):\n",
    "        if topic_index == target_topic_idx:\n",
    "            topic_certainties = certainty_Y_cpy[topic_index]\n",
    "            percentile = np.percentile(topic_certainties, 10)\n",
    "            print('precentile', percentile,)\n",
    "            selected = np.where(topic_certainties <= percentile)\n",
    "            selected_batch_indices = this_batch_indices[selected]\n",
    "            selected_X = train_X[selected_batch_indices].tolist()\n",
    "            selected_Y = train_Y[selected_batch_indices].tolist()\n",
    "            total_X += selected_X\n",
    "            total_Y += selected_Y\n",
    "\n",
    "    eval_model(model_copy, test_X, test_Y)\n",
    "    batch_idx += 1\n",
    "\n",
    "    \n",
    "\n",
    "print('Done!')\n",
    "# print(uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.1  0.2  0.2  0.   0.2]\n",
      " [ 0.   0.3  0.2  0.   0.   0. ]\n",
      " [ 0.   0.1  0.   0.2  0.1  0.1]\n",
      " ..., \n",
      " [ 0.1  0.   0.1  0.3  0.1  0.1]\n",
      " [ 0.   0.   0.1  0.   0.1  0.2]\n",
      " [ 0.   0.   0.3  0.   0.   0.1]]\n"
     ]
    }
   ],
   "source": [
    "print(uncertainty_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "sample size,precision,recall,prior\n",
      "455, 0.66, 0.27, 0.09\n",
      "41, 0.00, 0.00, 0.01\n",
      "243, 0.63, 0.30, 0.05\n",
      "13, 0.00, 0.00, 0.00\n",
      "209, 0.57, 0.24, 0.04\n",
      "40, 1.00, 0.03, 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# random selection strategy\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from sklearn.metrics.classification import precision_score, recall_score\n",
    "\n",
    "batch_size = 5000\n",
    "batch_idx = 0\n",
    "train_test_split = 0.9\n",
    "divider = int(len(tokenized_sequence) * train_test_split)\n",
    "train_X, train_Y = tokenized_sequence[:divider], label_matrix[:divider]\n",
    "test_X, test_Y = tokenized_sequence[divider:], label_matrix[divider:]\n",
    "total_data = train_X.shape[0]\n",
    "random_selected_idx = np.random.choice(range(total_data), num_samples//10, replace=False)\n",
    "sim = 10\n",
    "\n",
    "model_copy.fit(\n",
    "    train_X[random_selected_idx], \n",
    "    train_Y[random_selected_idx], \n",
    "    epochs=50, batch_size=100, verbose=0)\n",
    "eval_model(model_copy, test_X, test_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
