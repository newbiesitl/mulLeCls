{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from data set\n",
    "from utils.data_utils import jigsaw_toxix_ds_get_df\n",
    "import numpy as np\n",
    "from config import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = jigsaw_toxix_ds_get_df()\n",
    "comments = df[\"comment_text\"].tolist()\n",
    "comments = [x.replace('\\n', ' ') for x in comments]\n",
    "\n",
    "classes = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "dump_text = '\\n'.join(comments)\n",
    "total_classes = 6\n",
    "class_matrix = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].astype('int')\n",
    "label_matrix = class_matrix.values\n",
    "h_dim = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained w2v...done.\n"
     ]
    }
   ],
   "source": [
    "# load word2vec model\n",
    "import os \n",
    "from config import model_folder\n",
    "from model_factory.embeddings.w2v import w2v_load_from_keyedvectors, build_embedding_layer\n",
    "w2v_name = 'google_keyed_vector_format.bin'\n",
    "model_path = os.path.join(model_folder, w2v_name)\n",
    "print('loading pretrained w2v', end='...')    \n",
    "w2v_model = w2v_load_from_keyedvectors(model_path)\n",
    "vocab = w2v_model.vocab\n",
    "print('done.')\n",
    "word_2_idx = dict(zip(vocab.keys(), range(len(vocab))))\n",
    "max_sent_length = 80\n",
    "trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing data...done.\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "import nltk\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def tokenizer(text, word2idx, max_len=80, total=None):\n",
    "    if total is not None:\n",
    "        text = text[:total]\n",
    "    for sentence_idx in range(len(text)):\n",
    "        sentence = text[sentence_idx]\n",
    "        text[sentence_idx] = nltk.word_tokenize(sentence)\n",
    " \n",
    "    def _sent_to_idx(s, w2i):\n",
    "        for word_idx in range(len(s)):\n",
    "            word = s[word_idx]\n",
    "            idx = w2i.get(word, w2i['null'])\n",
    "            s[word_idx] = idx\n",
    "        return s\n",
    "        \n",
    "    for sentence_idx in range(len(text)):\n",
    "        sentence = text[sentence_idx]\n",
    "        sequence = _sent_to_idx(sentence, word2idx)\n",
    "        text[sentence_idx] = sequence\n",
    "    \n",
    "    text = pad_sequences(text, maxlen=max_len, value=word2idx['null'])   \n",
    "    return text\n",
    "\n",
    "num_samples = 50000\n",
    "print('tokenizing data', end='...')\n",
    "tokenized_sequence = tokenizer(comments, word_2_idx, max_sent_length, total=num_samples)\n",
    "if num_samples is not None:\n",
    "    label_matrix = class_matrix.values[:num_samples]\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize embedding layer...done.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 80, 300)           900000000 \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "alpha_dropout_1 (AlphaDropou (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1806      \n",
      "=================================================================\n",
      "Total params: 900,813,306\n",
      "Trainable params: 813,306\n",
      "Non-trainable params: 900,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# bayes network\n",
    "\n",
    "from model_factory.lstm_mc_dropout import SeqCLS\n",
    "from keras import backend as K\n",
    "\n",
    "def get_new_model(w2v_model):\n",
    "    print('initialize embedding layer', end='...')\n",
    "    embedding_layer = build_embedding_layer(w2v_model, word_2_idx, \n",
    "                                            len(vocab), max_sent_length, trainable)\n",
    "    print('done.')\n",
    "    m = SeqCLS()\n",
    "    m.configure(None, \n",
    "                None, \n",
    "                total_classes, 300, \n",
    "                pretrained_embedding=embedding_layer,\n",
    "                verbose=1,\n",
    "                )\n",
    "    return m\n",
    "\n",
    "\n",
    "            \n",
    "model_copy = get_new_model(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def reset_weights():\n",
    "    K.get_session().close()\n",
    "    K.set_session(tf.Session())\n",
    "    K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "                \n",
    "reset_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "0.24638910567,0.5\n",
      "sample size,precision,recall,prior\n",
      "455, 0.10, 0.53, 0.09\n",
      "41, 0.01, 0.46, 0.01\n",
      "243, 0.05, 0.47, 0.05\n",
      "13, 0.00, 0.46, 0.00\n",
      "209, 0.04, 0.46, 0.04\n",
      "40, 0.01, 0.50, 0.01\n",
      "Epoch 1/20\n",
      " - 1s - loss: 0.7874\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.3107\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.1711\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.1481\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.1542\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.1499\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.1460\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.1437\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.1377\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.1392\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.1386\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.1400\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.1397\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.1399\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.1337\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.1353\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.1357\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.1328\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.1341\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.1328\n",
      "0.0514090713127,0.5\n",
      "sample size,precision,recall,prior\n",
      "455, 0.00, 0.00, 0.09\n",
      "41, 0.00, 0.00, 0.01\n",
      "243, 0.00, 0.00, 0.05\n",
      "13, 0.00, 0.00, 0.00\n",
      "209, 0.00, 0.00, 0.04\n",
      "40, 0.00, 0.00, 0.01\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.5543\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.1794\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.1638\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.1557\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.1543\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.1476\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.1457\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.1474\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.1476\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.1460\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.1476\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.1454\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.1446\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.1431\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.1429\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.1425\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.1403\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.1429\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.1418\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.1435\n",
      "0.0290513814576,0.5\n",
      "sample size,precision,recall,prior\n",
      "455, 0.00, 0.00, 0.09\n",
      "41, 0.00, 0.00, 0.01\n",
      "243, 0.00, 0.00, 0.05\n",
      "13, 0.00, 0.00, 0.00\n",
      "209, 0.00, 0.00, 0.04\n",
      "40, 0.00, 0.00, 0.01\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/pintellect/anaconda3/envs/ml-dev/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.4217\n",
      "Epoch 2/20\n",
      " - 2s - loss: 0.1594\n",
      "Epoch 3/20\n",
      " - 2s - loss: 0.1464\n",
      "Epoch 4/20\n",
      " - 2s - loss: 0.1453\n",
      "Epoch 5/20\n",
      " - 2s - loss: 0.1414\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from sklearn.metrics.classification import precision_score, recall_score\n",
    "\n",
    "batch_size = 5000\n",
    "batch_idx = 0\n",
    "train_test_split = 0.9\n",
    "divider = int(len(tokenized_sequence) * train_test_split)\n",
    "train_X, train_Y = tokenized_sequence[:divider], label_matrix[:divider]\n",
    "test_X, test_Y = tokenized_sequence[divider:], label_matrix[divider:]\n",
    "total_data = train_X.shape[0]\n",
    "sim = 10\n",
    "total_X = []\n",
    "total_Y = []\n",
    "def eval_model(m, test_X, test_Y, sim=10):\n",
    "    pred_Y, uncertainty_Y = m.predict_with_uncertainty(test_X, sim=sim)\n",
    "    topic_Y = test_Y.T\n",
    "    pred_topic_Y = pred_Y.T\n",
    "    # evaluate performance\n",
    "    threshold = 0.5\n",
    "    print(threshold)\n",
    "    print(','.join(['sample size','precision','recall', 'prior']))\n",
    "    # print(pred_topic_Y[0].tolist())\n",
    "    for topic_idx in range(len(pred_topic_Y)):\n",
    "        true_topic = topic_Y[topic_idx]\n",
    "        pred_topic = pred_topic_Y[topic_idx]\n",
    "        pred_topic[np.where(pred_topic >= threshold)] = 1\n",
    "        pred_topic[np.where(pred_topic < threshold)] = 0\n",
    "        print('%d, %.2f, %.2f, %.2f' \n",
    "              % (sum(true_topic), \n",
    "                 precision_score(true_topic, pred_topic, average='binary'), \n",
    "                 recall_score(true_topic, pred_topic), \n",
    "                 sum(true_topic)/len(true_topic)))\n",
    "\n",
    "print(batch_size)\n",
    "target_topic_idx = 0\n",
    "\n",
    "while True:\n",
    "    l, r = batch_idx*batch_size, min((batch_idx+1)*batch_size, total_data-1)\n",
    "    if l > r:\n",
    "        batch_idx = 0\n",
    "        break\n",
    "    this_batch_indices = np.array(range(l, r))\n",
    "    selected_batch = train_X[this_batch_indices]\n",
    "    reset_weights()\n",
    "    if len(total_X) > 0:        \n",
    "        model_copy.fit(\n",
    "            np.array(total_X), \n",
    "            np.array(total_Y), \n",
    "            epochs=20, batch_size=100, verbose=2)\n",
    "    pred_Y, uncertainty_Y = model_copy.predict_with_uncertainty(selected_batch, sim=sim)\n",
    "    \n",
    "\n",
    "    uncertainty_Y_cpy = np.swapaxes(uncertainty_Y, 0, -1)\n",
    "    \n",
    "    for topic_index in range(uncertainty_Y_cpy.shape[0]):\n",
    "        if topic_index == target_topic_idx:\n",
    "            topic_uncertainties = uncertainty_Y_cpy[topic_index]\n",
    "            percentile = np.percentile(topic_uncertainties, 90)\n",
    "            print(percentile, end=',')\n",
    "            selected = np.where(topic_uncertainties > percentile)\n",
    "            selected_batch_indices = this_batch_indices[selected]\n",
    "            selected_X = train_X[selected_batch_indices].tolist()\n",
    "            selected_Y = train_Y[selected_batch_indices].tolist()\n",
    "            total_X += selected_X\n",
    "            total_Y += selected_Y\n",
    "\n",
    "    eval_model(model_copy, test_X, test_Y)\n",
    "    batch_idx += 1\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# print(uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
